<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
        "http://www.w3.org/TR/html4/loose.dtd">
<html>
<meta name="GENERATOR" content="TtH 3.67">
 <style type="text/css"> div.p { margin-top: 7pt;}</style>
 <style type="text/css"><!--
 td div.comp { margin-top: -0.6ex; margin-bottom: -1ex;}
 td div.comb { margin-top: -0.6ex; margin-bottom: -.6ex;}
 td div.hrcomp { line-height: 0.9; margin-top: -0.8ex; margin-bottom: -1ex;}
 td div.norm {line-height:normal;}
 span.roman {font-family: serif; font-style: normal; font-weight: normal;} 
 span.overacc2 {position: relative;  left: .8em; top: -1.2ex;}
 span.overacc1 {position: relative;  left: .6em; top: -1.2ex;} --></style>


<title> Cluster Analysis</title>
 
<h1 align="center">Cluster Analysis </h1>



 <h2><a name="tth_sEc1">
1</a>&nbsp;&nbsp;Clustering Techniques</h2>
Much of the history of cluster analysis is concerned with developing algorithms that 
were not too computer intensive, since early computers were not nearly as powerful as
they are today.  Accordingly, computational shortcuts have traditionally been used in 
many cluster analysis algorithms.  These algorithms have proven to be very useful, and
can be found in most computer software.  

<div class="p"><!----></div>
More recently, many of these older methods have been revisited and updated to reflect 
the fact that certain computations that once would have overwhelmed the available
computers can now be performed routinely.  In R, a number of these updated versions of
cluster analysis algorithms are available through the <tt>cluster</tt> library, providing
us with a large selection of methods to perform cluster analysis, and the possibility of
comparing the old methods with the new to see if they really provide an advantage.

<div class="p"><!----></div>
One of the oldest methods of cluster analysis is known as k-means cluster analysis, and 
is available in R through the <tt>kmeans</tt> function.  The
first step (and certainly not a trivial one) when using k-means cluster analysis is to 
specify the number of clusters (k) that will be formed in the final solution.   The process
begins by choosing k observations to serve as centers for the clusters.  Then, the distance
from each of the other observations is calculated for each of the k clusters, and 
observations are put in the cluster to which they are the closest.   After each observation
has been put in a cluster, the center of the clusters is recalculated, and every observation
is checked to see if it might be closer to a different cluster, now that the centers have
been recalculated.   The process continues until no observations switch clusters.

<div class="p"><!----></div>
Looking on the good side, the k-means technique is fast, and doesn't require calculating all
of the distances between each observation and every other observation.  It can be written
to efficiently deal with very large data sets, so it may be useful in cases where other 
methods fail.  On the down side, if you rearrange your data, it's very possible that you'll
get a different solution every time you change the ordering of your data.  Another criticism
of this technique is that you may try, for example, a 3 cluster solution that seems to work
pretty well, but when you look for the 4 cluster solution, all of the structure that the 
3 cluster solution revealed is gone.  This makes the procedure somewhat unattractive if you
don't know exactly how many clusters you should have in the first place. 

<div class="p"><!----></div>
The R <tt>cluster</tt> library provides a modern alternative to k-means clustering, known
as <tt>pam</tt>, which is an acronym for "Partitioning around Medoids".  The term 
medoid refers to an observation within a cluster for which the sum of the distances between
it and all the other members of the cluster is a minimum.  <tt>pam</tt> requires that you
know the number of clusters that you want (like k-means clustering), but it does more 
computation than k-means in order to insure that the medoids it finds are truly representative
of the observations within a given cluster.  Recall that in the k-means method the centers of 
the clusters (which might or might not actually correspond to a particular observation) are
only recaculated after all of the observations have had a chance to move from one cluster to
another.  With <tt>pam</tt>, the sums of the distances between objects within a cluster are 
constantly recalculated as observations move around, which will hopefully provide a more 
reliable solution.   Furthermore, as a by-product of the clustering operation it identifies 
the observations that represent the medoids, and these observations (one per cluster) can
be considered a representative example of the members of that cluster which may be useful in
some situations.  <tt>pam</tt> does require that the entire distance matrix is calculated to
facilitate the recalculation of the medoids,  and it does involve considerably more 
computation than k-means, but with modern computers this may not be a important consideration.
As with k-means, there's no guarantee that the structure that's revealed with a small 
number of clusters will be retained when you increase the number of clusters. 

<div class="p"><!----></div>
Another class of clustering methods, known as hierarchical agglomerative clustering methods,  
starts out by putting each observation into its own separate cluster.  
It then examines 
all the distances between all the observations and pairs together the two closest ones
to form a new cluster.   
This is a 
simple operation, since hierarchical methods require a distance matrix, and it represents
exactly what we want - the distances between individual observations.  So finding the 
first cluster to form simply means looking for the smallest number in the distance matrix
and joining the two observations that the distance correspnds to into a new cluster.
Now there is one less cluster than there are observations.  To 
determine which observations will form the next cluster, we need to come up with a 
method for finding the distance between an existing cluster and individual observations,
since once a cluster has been formed, we'll determine which observation will join it based
on the distance between the cluster and the observation.
Some of the methods that have been proposed to do this are to take the minimum distance
between an observation and any member of the cluster, to take the maximum distance, to 
take the average distance, or to use some kind of measure that minimizes the distances 
between observations within the cluster.  Each of these methods will reveal certain types
of structure within the data.  Using the minimum tends to find clusters that are drawn out
and "snake"-like, while using the maximum tends to find compact clusters.  Using the 
mean is a compromise between those methods.  One method that tends to produce clusters
of more equal size is known as Ward's method.  It attempts to form clusters keeping the 
distances within the clusters as small as possible, and is often useful when the other 
methods find clusters with only a few observations.   Agglomerative Hierarchical cluster
analysis is provided in R through the <tt>hclust</tt> function.

<div class="p"><!----></div>
Notice that, by its very nature, solutions with many clusters are nested within the 
solutions that have fewer clusters, so observations don't "jump ship" as they do 
in k-means or the <tt>pam</tt> methods.  Furthermore, we don't need to tell these 
procedures how many clusters we want - we get a complete set of solutions starting from
the trivial case of each observation in a separate cluster all the way to the other 
trivial case where we say all the observations are in a single cluster.

<div class="p"><!----></div>
Traditionally, hierarchical cluster analysis has taken computational shortcuts when updating
the distance matrix to reflect new clusters.  In particular, when a new cluster is formed
and the distance matrix is updated, all the information about the individual members of the 
cluster is discarded in order to make the computations faster.  The <tt>cluster</tt> library
provides the <tt>agnes</tt> function which uses essentially the same technique as <tt>hclust</tt>,
but which uses fewer shortcuts when updating the distance matrix.  For example, when the 
mean method of calculating
the distance between observations and clusters is used, <tt>hclust</tt> only uses the two
observations and/or clusters which were recently merged when updating the distance matrix,
while <tt>agnes</tt> calculates those distances as the average of all the distances between
all the observations in the two clusters.  While the two techniques will usually agree quite
closely when minimum or maximum updating methods are used, there may be noticeable differences
when updating using the average distance or Ward's method.

<div class="p"><!----></div>
 <h2><a name="tth_sEc2">
2</a>&nbsp;&nbsp;Hierarchial Clustering</h2>
For the hierarchial clustering methods, the dendogram is the main 
graphical tool for getting insight into a cluster solution.  When
you use <tt>hclust</tt> or <tt>agnes</tt> to perform a cluster analysis,
you can see the dendogram by passing the result of the clustering to 
the <tt>plot</tt> function.   

<div class="p"><!----></div>
To illustrate interpretation of the dendogram, we'll look at a cluster 
analysis performed on a set of cars from 1978-1979; the data can be found at
<a href="http://www.stat.berkeley.edu/classes/s133/data/cars.tab">http://www.stat.berkeley.edu/classes/s133/data/cars.tab</a>.  Since the data
is a tab-delimited file, we use <tt>read.delim</tt>:

<pre>
&#62;&nbsp;cars&nbsp;=&nbsp;read.delim('cars.tab',stringsAsFactors=FALSE)

</pre>

<div class="p"><!----></div>
To get an idea of what information we have, let's look at the first few records;

<div class="p"><!----></div>

<pre>&nbsp;
&#62;&nbsp;head(cars)&nbsp;
&nbsp;&nbsp;Country&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Car&nbsp;&nbsp;MPG&nbsp;Weight&nbsp;Drive_Ratio&nbsp;Horsepower
1&nbsp;&nbsp;&nbsp;&nbsp;U.S.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Buick&nbsp;Estate&nbsp;Wagon&nbsp;16.9&nbsp;&nbsp;4.360&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.73&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;155
2&nbsp;&nbsp;&nbsp;&nbsp;U.S.&nbsp;Ford&nbsp;Country&nbsp;Squire&nbsp;Wagon&nbsp;15.5&nbsp;&nbsp;4.054&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.26&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;142
3&nbsp;&nbsp;&nbsp;&nbsp;U.S.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chevy&nbsp;Malibu&nbsp;Wagon&nbsp;19.2&nbsp;&nbsp;3.605&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.56&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;125
4&nbsp;&nbsp;&nbsp;&nbsp;U.S.&nbsp;&nbsp;&nbsp;&nbsp;Chrysler&nbsp;LeBaron&nbsp;Wagon&nbsp;18.5&nbsp;&nbsp;3.940&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.45&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;150
5&nbsp;&nbsp;&nbsp;&nbsp;U.S.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chevette&nbsp;30.0&nbsp;&nbsp;2.155&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.70&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;68
6&nbsp;&nbsp;&nbsp;Japan&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Toyota&nbsp;Corona&nbsp;27.5&nbsp;&nbsp;2.560&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.05&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;95
&nbsp;&nbsp;Displacement&nbsp;Cylinders
1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;350&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8
2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;351&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8
3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;267&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8
4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;360&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8
5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;98&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4
6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;134&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4

</pre>
 It looks like the variables are measured on different scales, so we will
likely want to standardize the data before proceeding.  The <tt>daisy</tt> function
in the <tt>cluster</tt> library will automatically perform standardization, but it
doesn't give you complete control.  If you have a particular method of standardization
in mind, you can use the <tt>scale</tt> function.  You pass <tt>scale</tt> a matrix
or data frame
to be standardized, and two optional vectors.    The first, called <tt>center</tt>,
is a vector of values, one for each column of the matrix or data frame to be 
standardized, which will
be subtracted from every entry in that column.  The second, called <tt>scale</tt>, is
similar to <tt>center</tt>, but is used to divide the values in each column.  Thus,
to get z-scores, you could pass scale a vector of means for <tt>center</tt>, and 
a vector of standard deviations for <tt>scale</tt>.  These vectors can be created with
the <tt>apply</tt> function, that performs the same operation on each row or column of 
a matrix.  Suppose we want to standardize by subtracting the median and dividing by 
the mean average deviation:

<pre>
&#62;&nbsp;cars.use&nbsp;=&nbsp;cars[,-c(1,2)]
&#62;&nbsp;medians&nbsp;=&nbsp;apply(cars.use,2,median)
&#62;&nbsp;mads&nbsp;=&nbsp;apply(cars.use,2,mad)
&#62;&nbsp;cars.use&nbsp;=&nbsp;scale(cars.use,center=medians,scale=mads)

</pre>
  (The 2 used as the second argument to apply means to apply the function to
the columns of the matrix or data frame; a value of 1 means to use the rows.)
The country of origin and name of the car will not be useful in the 
cluster analysis, so they have been removed.  Notice that the <tt>scale</tt> 
function doesn't change the order of the rows of the data frame, so it will be easy
to identify observations using the omitted columns from the original data.

<div class="p"><!----></div>
First, we'll take a look at a hierarchical method, since it will provide information
about solutions with different numbers of clusters.   The first step is calculating
a distance matrix.  
For a data set with <tt>n</tt> observations, the distance matrix will have 
<tt>n</tt> rows and <tt>n</tt> columns; the <tt>(i,j)</tt>th element of the 
distance matrix will be the difference between observation <tt>i</tt> and 
observation <tt>j</tt>.
There are two functions that can be used to calculate distance
matrices in R; the <tt>dist</tt> function, which is included in every version of R,
and the <tt>daisy</tt> function, which is part of the <tt>cluster</tt> library.  
We'll use the <tt>dist</tt> function in this example, but you should familiarize 
yourself with the <tt>daisy</tt> function (by reading its help page), since it offers
some capabilities that <tt>dist</tt> does not.  Each function provides a choice of 
distance metrics; in this example, we'll use the default of Euclidean distance, but 
you may find that using other metrics will give different insights into the structure of 
your data.  

<pre>
cars.dist&nbsp;=&nbsp;dist(cars.use)

</pre>
  If you display the distance matrix in R (for example, by typing its name),
you'll notice that only the lower triangle of the matrix is displayed.  This is to 
remind us that the distance matrix is symmetric,  since it doesn't matter which 
observation we consider first when we calculate a distance.  R takes advantage of this
fact by only storing the lower triangle of the distance matrix.  All of the clustering
functions will recognize this and have no problems, but if you try to access the 
distance matrix in the usual way (for example, with subscripting), you'll see an
error message.    Thus, if you need to use the distance matrix with anything other than
the clustering functions, you'll need to use <tt>as.matrix</tt> to convert it to a 
regular matrix. 

<div class="p"><!----></div>
To get started, we'll use the <tt>hclust</tt> method;  the <tt>cluster</tt> library
provides a similar function, called <tt>agnes</tt> to perform hierarchical cluster 
analysis.  

<pre>
&#62;&nbsp;cars.hclust&nbsp;=&nbsp;hclust(cars.dist)

</pre>
 Once again, we're using the default method of <tt>hclust</tt>, which is
to update the distance matrix using what R calls "complete" linkage.  Using this
method, when a cluster is formed, its distance to other objects is computed as 
the maximum distance between any object in the cluster and the other object.  Other
linkage methods will provide different solutions, and should not be ignored.  For
example, using <tt>method=ward</tt> tends to produce clusters of fairly equal size,
and can be useful when other methods find clusters that contain just a few observations.

<div class="p"><!----></div>
Now that we've got a cluster solution (actually a collection of cluster solutions), 
how can we examine the results?  The main graphical tool for looking at a 
hierarchical cluster solution is known as a dendogram.  This is a tree-like display
that lists the objects which are clustered along the x-axis, and the distance at 
which the cluster was formed along the y-axis.  
(Distances along the x-axis are meaningless in a dendogram; the observations are
equally spaced to make the dendogram easier to read.)
To create a dendogram from a cluster
solution, simply pass it to the <tt>plot</tt> function.  The result is displayed
below.

<pre>
plot(cars.hclust,labels=cars$Car,main='Default&nbsp;from&nbsp;hclust')

</pre>
<img src="dendo1.png">

<div class="p"><!----></div>
If you choose any height along the y-axis of the dendogram, and move across the 
dendogram counting the number of lines that you cross, each line represents a 
group that was identified when objects were joined together into clusters.  The 
observations in that group are represented by the branches of the dendogram that
spread out below the line.  For example, if we look at a height of 6, and move 
across the x-axis at that height, we'll cross two lines.  That defines a two-cluster
solution;  by following the line down through all its branches, we can see the names
of the cars that are included in these two clusters.  Since the y-axis represents
how close together observations were when they were merged into clusters, clusters
whose branches are very close together (in terms of the heights at which they were
merged) probably aren't very reliable.  But if there's a big difference 
along the y-axis 
between 
the last merged cluster and the currently merged one, that indicates that the 
clusters formed are probably doing a good job in showing us the structure of the 
data.  Looking at the dendogram for the car data, there are clearly two very 
distinct groups; the right hand group seems to consist of two more distinct 
cluster, while most of the observations in the left hand group are clustering together
at about the same height
For this data set, it looks like
either two or three groups might be an interesting place to start investigating.
This is not to imply that looking at solutions with more clusters would
be meaningless, but the data seems to suggest that two or three clusters might be
a good start.  For a problem of this size, we can see the names of the cars, so we
could start interpreting the results immediately from the dendogram, but when there
are larger numbers of observations, this won't be possible. 

<div class="p"><!----></div>
One of the first things we can look at is how many cars are in each of the groups.
We'd like to do this for both the two cluster and three cluster solutions.  You can
create a vector showing the cluster membership of each observation by using the 
<tt>cutree</tt> function.  Since the object returned by a hierarchical cluster 
analysis contains information about solutions with different numbers of clusters,
we pass the <tt>cutree</tt> function the cluster object and the number of clusters
we're interested in.  So to get cluster memberships for the three cluster solution,
we could use:

<pre>
&#62;&nbsp;groups.3&nbsp;=&nbsp;cutree(cars.hclust,3)

</pre>
 Simply displaying the group memberships isn't that revealing.  A good
first step is to use the <tt>table</tt> function to see how many observations
are in each cluster.  We'd like a solution where there aren't too many clusters 
with just a few observations, because it may make it difficult to interpret our
results.  For the three cluster solution, the distribution among the clusters looks
good:

<pre>
&#62;&nbsp;table(groups.3)
groups.3
&nbsp;1&nbsp;&nbsp;2&nbsp;&nbsp;3
&nbsp;8&nbsp;20&nbsp;10

</pre>
  Notice that you can get this information for many different groupings
at once by combining the calls to <tt>cutree</tt> and <tt>table</tt> in a call
to <tt>sapply</tt>.  For example, to see the sizes of the clusters for solutions
ranging from 2 to 6 clusters, we could use:

<pre>
&#62;&nbsp;counts&nbsp;=&nbsp;sapply(2:6,function(ncl)table(cutree(cars.hclust,ncl)))
&#62;&nbsp;names(counts)&nbsp;=&nbsp;2:6
&#62;&nbsp;counts
$"2"

&nbsp;1&nbsp;&nbsp;2
18&nbsp;20

$"3"

&nbsp;1&nbsp;&nbsp;2&nbsp;&nbsp;3
&nbsp;8&nbsp;20&nbsp;10

$"4"

&nbsp;1&nbsp;&nbsp;2&nbsp;&nbsp;3&nbsp;&nbsp;4
&nbsp;8&nbsp;17&nbsp;&nbsp;3&nbsp;10

$"5"

&nbsp;1&nbsp;&nbsp;2&nbsp;&nbsp;3&nbsp;&nbsp;4&nbsp;&nbsp;5
&nbsp;8&nbsp;11&nbsp;&nbsp;6&nbsp;&nbsp;3&nbsp;10

$"6"

&nbsp;1&nbsp;&nbsp;2&nbsp;&nbsp;3&nbsp;&nbsp;4&nbsp;&nbsp;5&nbsp;&nbsp;6
&nbsp;8&nbsp;11&nbsp;&nbsp;6&nbsp;&nbsp;3&nbsp;&nbsp;5&nbsp;&nbsp;5


</pre>

<div class="p"><!----></div>
To see which cars are in which clusters, we can use subscripting on the 
vector of car names to choose just the observations from a particular cluster.  
Since we used all of the observations in the data set to form the distance
matrix, the ordering of the names in the original data will coincide with the
values returned by <tt>cutree</tt>.  If observations were removed from the data
before the distance matrix is computed, it's important to remember to make the 
same deletions in the vector from the original data set that will be used to 
identify observations.  So, to see which cars were in the first cluster for the 
four cluster solution, we 
can use:

<pre>
&#62;&nbsp;cars$Car[groups.3&nbsp;==&nbsp;1]
[1]&nbsp;Buick&nbsp;Estate&nbsp;Wagon&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ford&nbsp;Country&nbsp;Squire&nbsp;Wagon
[3]&nbsp;Chevy&nbsp;Malibu&nbsp;Wagon&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chrysler&nbsp;LeBaron&nbsp;Wagon
[5]&nbsp;Chevy&nbsp;Caprice&nbsp;Classic&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ford&nbsp;LTD
[7]&nbsp;Mercury&nbsp;Grand&nbsp;Marquis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dodge&nbsp;St&nbsp;Regis

</pre>
 As usual, if we want to do the same thing for all the groups at once,
we can use <tt>sapply</tt>:

<pre>
&#62;&nbsp;sapply(unique(groups.3),function(g)cars$Car[groups.3&nbsp;==&nbsp;g])
[[1]]
[1]&nbsp;Buick&nbsp;Estate&nbsp;Wagon&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ford&nbsp;Country&nbsp;Squire&nbsp;Wagon
[3]&nbsp;Chevy&nbsp;Malibu&nbsp;Wagon&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chrysler&nbsp;LeBaron&nbsp;Wagon
[5]&nbsp;Chevy&nbsp;Caprice&nbsp;Classic&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ford&nbsp;LTD
[7]&nbsp;Mercury&nbsp;Grand&nbsp;Marquis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dodge&nbsp;St&nbsp;Regis

[[2]]
&nbsp;[1]&nbsp;Chevette&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Toyota&nbsp;Corona&nbsp;&nbsp;&nbsp;&nbsp;Datsun&nbsp;510&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dodge&nbsp;Omni
&nbsp;[5]&nbsp;Audi&nbsp;5000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Saab&nbsp;99&nbsp;GLE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ford&nbsp;Mustang&nbsp;4&nbsp;&nbsp;&nbsp;Mazda&nbsp;GLC
&nbsp;[9]&nbsp;Dodge&nbsp;Colt&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AMC&nbsp;Spirit&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VW&nbsp;Scirocco&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Honda&nbsp;Accord&nbsp;LX
[13]&nbsp;Buick&nbsp;Skylark&nbsp;&nbsp;&nbsp;&nbsp;Pontiac&nbsp;Phoenix&nbsp;&nbsp;Plymouth&nbsp;Horizon&nbsp;Datsun&nbsp;210
[17]&nbsp;Fiat&nbsp;Strada&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VW&nbsp;Dasher&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BMW&nbsp;320i&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VW&nbsp;Rabbit

[[3]]
&nbsp;[1]&nbsp;Volvo&nbsp;240&nbsp;GL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Peugeot&nbsp;694&nbsp;SL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Buick&nbsp;Century&nbsp;Special
&nbsp;[4]&nbsp;Mercury&nbsp;Zephyr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dodge&nbsp;Aspen&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AMC&nbsp;Concord&nbsp;D/L
&nbsp;[7]&nbsp;Ford&nbsp;Mustang&nbsp;Ghia&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chevy&nbsp;Citation&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Olds&nbsp;Omega
[10]&nbsp;Datsun&nbsp;810


</pre>

<div class="p"><!----></div>
We could also see what happens when we use the four cluster solution

<pre>
&#62;&nbsp;groups.4&nbsp;=&nbsp;cutree(cars.hclust,4)
&#62;&nbsp;sapply(unique(groups.4),function(g)cars$Car[groups.4&nbsp;==&nbsp;g])
[[1]]
[1]&nbsp;Buick&nbsp;Estate&nbsp;Wagon&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ford&nbsp;Country&nbsp;Squire&nbsp;Wagon
[3]&nbsp;Chevy&nbsp;Malibu&nbsp;Wagon&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chrysler&nbsp;LeBaron&nbsp;Wagon
[5]&nbsp;Chevy&nbsp;Caprice&nbsp;Classic&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ford&nbsp;LTD
[7]&nbsp;Mercury&nbsp;Grand&nbsp;Marquis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dodge&nbsp;St&nbsp;Regis

[[2]]
&nbsp;[1]&nbsp;Chevette&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Toyota&nbsp;Corona&nbsp;&nbsp;&nbsp;&nbsp;Datsun&nbsp;510&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dodge&nbsp;Omni
&nbsp;[5]&nbsp;Ford&nbsp;Mustang&nbsp;4&nbsp;&nbsp;&nbsp;Mazda&nbsp;GLC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dodge&nbsp;Colt&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AMC&nbsp;Spirit
&nbsp;[9]&nbsp;VW&nbsp;Scirocco&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Honda&nbsp;Accord&nbsp;LX&nbsp;&nbsp;Buick&nbsp;Skylark&nbsp;&nbsp;&nbsp;&nbsp;Pontiac&nbsp;Phoenix
[13]&nbsp;Plymouth&nbsp;Horizon&nbsp;Datsun&nbsp;210&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fiat&nbsp;Strada&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VW&nbsp;Dasher
[17]&nbsp;VW&nbsp;Rabbit

[[3]]
[1]&nbsp;Audi&nbsp;5000&nbsp;&nbsp;&nbsp;Saab&nbsp;99&nbsp;GLE&nbsp;BMW&nbsp;320i

[[4]]
&nbsp;[1]&nbsp;Volvo&nbsp;240&nbsp;GL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Peugeot&nbsp;694&nbsp;SL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Buick&nbsp;Century&nbsp;Special
&nbsp;[4]&nbsp;Mercury&nbsp;Zephyr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dodge&nbsp;Aspen&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AMC&nbsp;Concord&nbsp;D/L
&nbsp;[7]&nbsp;Ford&nbsp;Mustang&nbsp;Ghia&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chevy&nbsp;Citation&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Olds&nbsp;Omega
[10]&nbsp;Datsun&nbsp;810


</pre>
The new cluster can be recognized as the third group in the above output.  

<div class="p"><!----></div>
Often there is an auxiliary variable in the original data set that was not included
in the cluster analysis, but may be of interest.  In fact, cluster analysis is 
sometimes performed to see if observations naturally group themselves in accord with
some already measured variable.  For this data set, we could ask whether the clusters
reflect the country of origin of the cars, stored in the variable <tt>Country</tt>
in the original data set.  The <tt>table</tt> function can be used, this time 
passing two arguments, to produce a cross-tabulation of cluster group membership
and country of origin:

<pre>
&#62;&nbsp;table(groups.3,cars$Country)

groups.3&nbsp;France&nbsp;Germany&nbsp;Italy&nbsp;Japan&nbsp;Sweden&nbsp;U.S.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;8
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;7
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;7
&#62;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

</pre>
Of interest is the fact that all of the cars in cluster 1 were manufactured in 
the US.  Considering the state of the automobile industry in 1978, and the cars
that were identified in cluster 1, this is not surprising. 

<div class="p"><!----></div>
In an example like this, with a small number of observations, we can often 
interpret the cluster solution directly by looking at the labels of the observations
that are in each cluster.  Of course, for larger data sets, this will be impossible
or meaningless.  A very useful method for characterizing clusters is to look at
some sort of summary statistic, like the median, of the variables that were used
to perform the cluster analysis, broken down by the groups that the cluster analysis
identified.  The <tt>aggregate</tt> function is well suited for this task, since it
will perform summaries on many variables simultaneously.  Let's look at the median
values for the variables we've used in the cluster analysis, broken up by the cluster
groups.  One oddity of the <tt>aggregate</tt> function is that it demands that the 
variable(s) used to divide up the data are passed to it in a list, even if there's 
only one variable:

<pre>
&#62;&nbsp;aggregate(cars.use,list(groups.3),median)
&nbsp;&nbsp;Group.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPG&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Weight&nbsp;Drive_Ratio&nbsp;Horsepower&nbsp;Displacement&nbsp;&nbsp;Cylinders
1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;-0.7945273&nbsp;&nbsp;1.5051136&nbsp;&nbsp;-0.9133729&nbsp;&nbsp;1.0476133&nbsp;&nbsp;&nbsp;&nbsp;2.4775849&nbsp;&nbsp;4.7214353
2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;0.6859228&nbsp;-0.5870568&nbsp;&nbsp;&nbsp;0.5269459&nbsp;-0.6027364&nbsp;&nbsp;&nbsp;-0.5809970&nbsp;-0.6744908
3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;-0.4058377&nbsp;&nbsp;0.5246039&nbsp;&nbsp;-0.1686227&nbsp;&nbsp;0.3587717&nbsp;&nbsp;&nbsp;&nbsp;0.3272282&nbsp;&nbsp;2.0234723


</pre>
 If the ranges of these numbers seem strange, it's because we standardized
the data before performing the cluster analysis.   While it is usually more 
meaningful to look at the variables in their original scales,  when data is 
centered, negative values mean "lower than most" and positive values mean
"higher than most".  Thus, group 1 is cars with relatively low MPG, 
high weight, low drive ratios, high horsepower and displacement, and more than
average number of cylinders.  Group 2 are cars with high gas mileage, and low weight
and horsepower; and group 3 is similar to group 1.
It may be easier to understand the groupings if we look at the variables in their
original scales:

<pre>
&#62;&nbsp;aggregate(cars[,-c(1,2)],list(groups.3),median)
&nbsp;&nbsp;Group.1&nbsp;&nbsp;&nbsp;MPG&nbsp;Weight&nbsp;Drive_Ratio&nbsp;Horsepower&nbsp;Displacement&nbsp;Cylinders
1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;17.30&nbsp;&nbsp;3.890&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.430&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;136.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;334&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8
2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;30.25&nbsp;&nbsp;2.215&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.455&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;79.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;105&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4
3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;20.70&nbsp;&nbsp;3.105&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.960&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;112.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;173&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6


</pre> 
It may also be useful to add the numbers of observations in each group to the 
above display.  Since <tt>aggregate</tt> returns a data frame, we can manipulate
it in any way we want:

<pre>
&#62;&nbsp;a3&nbsp;=&nbsp;aggregate(cars[,-c(1,2)],list(groups.3),median)
&#62;&nbsp;data.frame(Cluster=a3[,1],Freq=as.vector(table(groups.3)),a3[,-1])
&nbsp;&nbsp;Cluster&nbsp;Freq&nbsp;&nbsp;&nbsp;MPG&nbsp;Weight&nbsp;Drive_Ratio&nbsp;Horsepower&nbsp;Displacement&nbsp;Cylinders
1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;8&nbsp;17.30&nbsp;&nbsp;3.890&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.430&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;136.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;334&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8
2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;20&nbsp;30.25&nbsp;&nbsp;2.215&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.455&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;79.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;105&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4
3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;10&nbsp;20.70&nbsp;&nbsp;3.105&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.960&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;112.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;173&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6

</pre>
 To see how the four cluster solution differed from the three cluster solution,
we can perform the same analysis for that solution:

<pre>
&#62;&nbsp;a4&nbsp;=&nbsp;aggregate(cars[,-c(1,2)],list(groups.4),median)
&#62;&nbsp;data.frame(Cluster=a4[,1],Freq=as.vector(table(groups.4)),a4[,-1])
&nbsp;&nbsp;Cluster&nbsp;Freq&nbsp;&nbsp;MPG&nbsp;Weight&nbsp;Drive_Ratio&nbsp;Horsepower&nbsp;Displacement&nbsp;Cylinders
1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;8&nbsp;17.3&nbsp;&nbsp;3.890&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.43&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;136.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;334&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8
2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;17&nbsp;30.9&nbsp;&nbsp;2.190&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.37&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;75.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;98&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4
3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;21.5&nbsp;&nbsp;2.795&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.77&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;110.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;121&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4
4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;10&nbsp;20.7&nbsp;&nbsp;3.105&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.96&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;112.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;173&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6


</pre>
The main difference seems to be that the four cluster solution recognized a 
group of cars that have higher horsepower and drive ratios than the other
cars in the cluster they came from.

<div class="p"><!----></div>
 <h2><a name="tth_sEc3">
3</a>&nbsp;&nbsp;PAM: Partitioning Around Medoids</h2>
Unlike the hierarchical clustering methods, techniques like k-means cluster analysis
(available through the <tt>kmeans</tt> function) or 
partitioning around mediods (avaiable through the <tt>pam</tt> function 
in the <tt>cluster</tt> library) require that we specify
the number of clusters that will be formed  in advance.  <tt>pam</tt> offers some 
additional diagnostic information about a clustering solution, and provides a nice
example of an alternative technique to hierarchical clustering.  To use <tt>pam</tt>,
you must first load the <tt>cluster</tt> library.  You can pass <tt>pam</tt> a
data frame or a distance matrix; since we've already formed the distance matrix,
we'll use that.  
<tt>pam</tt>
also needs the number of clusters you wish to form.  Let's look at the three cluster
solution produced by <tt>pam</tt>:

<pre>
&#62;&nbsp;library(cluster)
&#62;&nbsp;cars.pam&nbsp;=&nbsp;pam(cars.dist,3)

</pre>
First of all, let's see if the <tt>pam</tt> solution agrees with the <tt>hclust</tt>
solution.  Since <tt>pam</tt> only looks at one cluster solution at a time, we don't
need to use the <tt>cutree</tt> function as we did with <tt>hclust</tt>; the cluster
memberships are stored in the <tt>clustering</tt> component of the <tt>pam</tt>
object; like most R objects, you can use the <tt>names</tt> function to see what 
else is available.  Further information can be found in the help page for 
<tt>pam.object</tt>.

<pre>
&#62;&nbsp;names(cars.pam)
[1]&nbsp;"medoids"&nbsp;&nbsp;&nbsp;&nbsp;"id.med"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"clustering"&nbsp;"objective"&nbsp;&nbsp;"isolation"
[6]&nbsp;"clusinfo"&nbsp;&nbsp;&nbsp;"silinfo"&nbsp;&nbsp;&nbsp;&nbsp;"diss"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"call"

</pre>
 We can use <tt>table</tt> to compare the results of the <tt>hclust</tt>
and <tt>pam</tt> solutions:

<pre>
&#62;&nbsp;table(groups.3,cars.pam$clustering)
groups.3&nbsp;&nbsp;1&nbsp;&nbsp;2&nbsp;&nbsp;3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;8&nbsp;&nbsp;0&nbsp;&nbsp;0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;0&nbsp;19&nbsp;&nbsp;1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;0&nbsp;&nbsp;0&nbsp;10

</pre>
The solutions seem to agree, except for 1 observations that <tt>hclust</tt> put
in group 2 and <tt>pam</tt> put in group 3.  Which observations was it?

<pre>
&#62;&nbsp;cars$Car[groups.3&nbsp;!=&nbsp;cars.pam$clustering]
[1]&nbsp;Audi&nbsp;5000

</pre>
Notice how
easy it is to get information like this due to the power of R's subscripting
operations. 

<div class="p"><!----></div>
One novel feature of <tt>pam</tt> is that it finds observations from the original
data that are typical of each cluster in the sense that they are closest to the 
center of the cluster.  The indexes of the medoids are stored in the <tt>id.med</tt>
component of the <tt>pam</tt> object, so we can use that component as a subscript
into the vector of car names  to see which ones were selected:

<pre>
&#62;&nbsp;cars$Car[cars.pam$id.med]
&#62;&nbsp;cars$Car[cars.pam$id.med]
[1]&nbsp;Dodge&nbsp;St&nbsp;Regis&nbsp;&nbsp;&nbsp;&nbsp;Dodge&nbsp;Omni&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ford&nbsp;Mustang&nbsp;Ghia

</pre>

<div class="p"><!----></div>
Another feature available with <tt>pam</tt> is a plot known as a silhouette plot.
First, a measure is calculated for each observation to see how well it fits into
the cluster that it's been assigned to.  This is done by comparing how close
the object is to other objects in its own cluster with how close it is to objects
in other clusters.  (A complete description can be found in the help page for 
<tt>silhouette</tt>.)  Values near one mean that the observation is well placed in
its cluster; values near 0 mean that it's likely that an observation might 
really belong in some other cluster.  Within each cluster, the value for this measure
is displayed from smallest to largest.  If the silhouette plot shows values close to
one for each observation, the fit was good; if there are many observations closer to
zero, it's an indication that the fit was not good.  The sihouette plot is very useful
in locating groups in a cluster analysis that may not be doing a good job; in turn
this information can be used to help select the proper number of clusters.  For the
current example, here's the silhouette plot for the three cluster <tt>pam</tt> solution,
produced by the command 

<pre>
&#62;&nbsp;plot(cars.pam)

</pre>

<div class="p"><!----></div>
<img src="sil.png">

<div class="p"><!----></div>
The plot indicates that there is a good structure to the clusters, with most 
observations seeming to belong to the cluster that they're in.
There is a summary measure at the bottom of the plot
labeled "Average Silhouette Width".  This table shows how to use the value: 

<center>

<div class="p"><!----></div>
<a name="tth_tAb1">
</a> 
<table border="1">
<tr><td>Range of SC</td><td>Interpretation</td></tr>
<tr><td>0.71-1.0</td><td>A strong structure has been found</td></tr>
<tr><td>0.51-0.70</td><td>A reasonable structure has been found</td></tr>
<tr><td>0.26-0.50</td><td>The structure is weak and could be artificial</td></tr>
<tr><td> &lt; 0.25</td><td>No substantial structure has been found</td></tr></table>


<div class="p"><!----></div>
</center>To create a silhouette plot for a particular solution derived from a hierarchical
cluster analysis, the <tt>silhouette</tt> function can be used.  This function
takes the appropriate output from <tt>cutree</tt> along with the distance matrix 
used for the clustering.  So to produce a silhouette plot for our 4 group 
hierarchical cluster (not shown), we could use the following statements:

<pre>
plot(silhouette(cutree(cars.hclust,4),cars.dist))

</pre>

<div class="p"><!----></div>
 <h2><a name="tth_sEc4">
4</a>&nbsp;&nbsp;AGNES: Agglomerative Nesting</h2>

<div class="p"><!----></div>
As an example of using the <tt>agnes</tt> function from the <tt>cluster</tt> package,
consider the famous Fisher iris data, available as the dataframe <tt>iris</tt> in R.
First let's look at some of the data:

<pre>
&#62;&nbsp;head(iris)
&nbsp;&nbsp;Sepal.Length&nbsp;Sepal.Width&nbsp;Petal.Length&nbsp;Petal.Width&nbsp;Species
1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2&nbsp;&nbsp;setosa
2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2&nbsp;&nbsp;setosa
3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2&nbsp;&nbsp;setosa
4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2&nbsp;&nbsp;setosa
5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2&nbsp;&nbsp;setosa
6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.4&nbsp;&nbsp;setosa

</pre>
We will only consider the numeric variables in the cluster analysis.
As mentioned previously, there are two functions to compute the 
distance matrix:  <tt>dist</tt> and <tt>daisy</tt>.  It should be 
mentioned that for data that's all numeric, using the function's 
defaults, the two methods will give the same answers.  We can
demonstrate this as follows:

<pre>
&#62;&nbsp;iris.use&nbsp;=&nbsp;subset(iris,select=-Species)
&#62;&nbsp;d&nbsp;=&nbsp;dist(iris.use)
&#62;&nbsp;library(cluster)
&#62;&nbsp;d1&nbsp;=&nbsp;daisy(iris.use)
&#62;&nbsp;sum(abs(d&nbsp;-&nbsp;d1))
[1]&nbsp;1.072170e-12

</pre>
 Of course, if we choose a non-default metric for
<tt>dist</tt>, the answers will be different:

<pre>
&#62;&nbsp;dd&nbsp;=&nbsp;dist(iris.use,method='manhattan')
&#62;&nbsp;sum(abs(as.matrix(dd)&nbsp;-&nbsp;as.matrix(d1)))
[1]&nbsp;38773.86

</pre>
The values are very different!

<div class="p"><!----></div>
Continuing with the cluster example, we can calculate
the cluster solution as follows:

<pre>
&#62;&nbsp;z&nbsp;=&nbsp;agnes(d)

</pre>
The plotting method for <tt>agnes</tt> objects presents two
different views of the cluster solution.  When we plot such
an object, the plotting function sets the graphics 
parameter <tt>ask=TRUE</tt>, and the following appears in your
R session each time a plot is to be drawn:

<pre>
Hit&nbsp;&lt;Return&#62;&nbsp;to&nbsp;see&nbsp;next&nbsp;plot:&nbsp;

</pre>
If you know you want a particular plot, you can pass the 
<tt>which.plots=</tt> argument an integer telling which 
plot you want.

<div class="p"><!----></div>
The first plot that is displayed is known as a banner plot.
The banner plot for the iris data is shown below:

<div class="p"><!----></div>
<img src="irisb.png">

<div class="p"><!----></div>
The white area on the left of the banner plot represents the 
unclustered data while the white lines that stick into the 
red are show the heights at which the clusters were formed. 
Since we don't want to include too many clusters that joined
together at similar heights, it looks like three clusters,
at a height of about 2 is a good solution.  It's clear from
the banner plot that if we lowered the height to, say 1.5,
we'd create a fourth cluster with only a few observations.

<div class="p"><!----></div>
The banner plot is just an alternative to the dendogram, which
is the second plot that's produced from an <tt>agnes</tt> object:

<div class="p"><!----></div>
<img src="irisd.png">

<div class="p"><!----></div>
The dendogram shows the same relationships, and it's a matter
of individual preference as to which one is easier to use.

<div class="p"><!----></div>
Let's see how well the clusters do in grouping the irises by species:

<div class="p"><!----></div>

<pre>
&#62;&nbsp;table(cutree(z,3),iris$Species)
&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;setosa&nbsp;versicolor&nbsp;virginica
&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;50&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0
&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;50&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;14
&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;36

</pre>
We were able to classify all the setosa and versicolor varieties correctly.
The following plot gives some insight into why we were so successful:

<div class="p"><!----></div>

<pre>
&#62;&nbsp;splom(~iris,groups=iris$Species,auto.key=TRUE)

</pre>

<div class="p"><!----></div>
<img src="iriss.png">

<div class="p"><!----></div>

<br /><br /><hr /><small>File translated from
T<sub><font size="-1">E</font></sub>X
by <a href="http://hutchinson.belmont.ma.us/tth/">
T<sub><font size="-1">T</font></sub>H</a>,
version 3.67.<br />On 16 Mar 2011, 15:27.</small>
</html>
